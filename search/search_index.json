{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>NeMo-Skills is a collection of pipelines to improve \"skills\" of large language models. We mainly focus on the ability to solve mathematical problems, but you can use our pipelines for many other tasks as well. Here are some of the things we support.</p> <ul> <li>Flexible inference: Seamlessly switch between API providers, local server and large-scale slurm jobs for LLM inference.</li> <li>Multiple formats: Use any of the NeMo, vLLM   and TensorRT-LLM servers and easily convert checkpoints from one format to another.</li> <li>Model evaluation: Evaluate your models on many popular benchmarks<ul> <li>Math problem solving: gsm8k, math, amc23, aime24, omni-math (and many more)</li> <li>Coding skills: human-eval, mbpp</li> <li>Chat/instruction following: ifeval, arena-hard</li> <li>General knowledge: mmlu (generative)</li> </ul> </li> <li>Model training: Train models at speed-of-light using NeMo-Aligner.</li> </ul> <p>To get started, follow the prerequisites and then run <code>ns --help</code> to see all available commands and their options.</p>"},{"location":"basics/common-parameters/","title":"Common parameters","text":"<p>Many of our scripts have a shared set of common parameters that we list here.</p>"},{"location":"basics/common-parameters/#all-pipeline-scripts","title":"All pipeline scripts","text":"<p>All scripts inside pipeline folder have the following parameters.</p> <ul> <li>--cluster: You always need to specify a cluster config that will be used to   control where the job is executed.</li> <li>--config_dir: By default we search for cluster configs inside <code>cluster_configs</code>   local folder, but you can control where they are located with this parameter.   You can also use <code>NEMO_SKILLS_CONFIG_DIR</code> environment variable for this purpose.</li> <li>--log_dir: Can be used to customize the location of slurm logs.</li> <li>--expname: You can always specify an experiment name, which is a   NeMo-Run concept. This will control where   the metadata is stored, the slurm job name and allows you to chain jobs one   after the other using the <code>--run_after</code> argument.</li> <li>--run_after: Can be used in conjunction with <code>--expname</code> to chain jobs to   run one after another (only applicable on slurm). E.g. run training job with   <code>--expname my-training-run</code> and then launch an eval with <code>--run_after my-training-run</code>.</li> <li>--partition: Can be used to run in a specific slurm partition (e.g. commonly used   to launch interactive jobs).</li> </ul>"},{"location":"basics/common-parameters/#generation-scripts","title":"Generation scripts","text":"<p>All of the scripts that involve LLM data generation accept a common set of parameters.</p> <ul> <li>--model: Either path to the model file or an API model name.</li> <li>--server_type: <code>nemo</code>, <code>trtllm</code>, <code>vllm</code> or <code>openai</code>. This is used on the client side   to correctly format a request to a particular server. This needs to match model   checkpoint format if self-hosting the model or has to be <code>openai</code> for both Nvidia   NIM API as well as the OpenAI API.</li> <li>--server_address: Only relevant for API models. E.g. use   <code>https://integrate.api.nvidia.com/v1</code> for Nvidia API and   <code>https://api.openai.com/v1</code> for OpenAI API.</li> <li>--server_gpus: Number of GPUs needed to host a model (only applicable to self-hosted models).</li> <li>--server_nodes: Number of nodes needed to host a model (only applicable to self-hosted models).</li> <li>--server_args: Any other arguments you need to pass to a corresponding server.   E.g. use <code>--server_args=\"--gpu-memory-utilization=0.99\"</code> to change gpu memory utilization of a   vLLM server.</li> </ul>"},{"location":"basics/inference/","title":"Inference","text":"<p>Here are the instructions on how to run inference with our repo.</p> <p>Make sure to complete prerequisites.</p>"},{"location":"basics/inference/#downloadconvert-the-model","title":"Download/convert the model","text":"<p>Get the model you want to use. You can use any model that's supported by vLLM, TensorRT-LLM or NeMo. You can also use Nvidia NIM API for models that are hosted there.</p> <p>Convert the model if it's not in the format you want to use. You do not need any conversion if using vLLM inference with HF models (and can directly use model id if you want vLLM to download it for you). For fastest inference we recommend to convert the model to TensorRT-LLM format.</p>"},{"location":"basics/inference/#start-the-server","title":"Start the server","text":"<p>Start the server hosting your model. Here is an example (make sure the <code>/hf_models</code> mount is defined in your cluster config). Skip this step if you want to use cloud models through an API.</p> <pre><code>ns start_server \\\n    --cluster local \\\n    --model /hf_models/Meta-Llama-3.1-8B-Instruct \\\n    --server_type vllm \\\n    --server_gpus 1 \\\n    --server_nodes 1\n</code></pre> <p>If the model needs to execute code, add <code>--with_sandbox</code></p>"},{"location":"basics/inference/#send-inference-requests","title":"Send inference requests","text":"<p>Click on  symbols in the snippet below to learn more details.</p> Self-hosted modelsAPI modelsWith code execution <pre><code>from nemo_skills.inference.server.model import get_model\nfrom nemo_skills.prompt.utils import get_prompt\n\nllm = get_model(server_type=\"vllm\")  # localhost by default\nprompt = get_prompt('generic/default', 'llama3-instruct') # (1)!\nprompts = [prompt.fill({'question': \"What's 2 + 2?\"})]\nprint(prompts[0]) # (2)!\noutputs = llm.generate(prompts=prompts)\nprint(outputs[0][\"generation\"]) # (3)!\n</code></pre> <ol> <li> <p>Here we use generic/default config      and llama3-instruct template.</p> <p>See nemo_skills/prompt for more config/template options  or create your own prompts</p> </li> <li> <p>This should print</p> <pre><code>&gt;&gt;&gt; print(prompts[0])\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nWhat's 2 + 2?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <p>If you don't want to use our prompt class, just create this string yourself</p> </li> <li> <p>This should print      <pre><code>&gt;&gt;&gt; print(outputs[0][\"generation\"])\n2 + 2 = 4.\n</code></pre></p> </li> </ol> <pre><code>from nemo_skills.inference.server.model import get_model\nfrom nemo_skills.prompt.utils import get_prompt\n\nllm = get_model( # (1)!\n    server_type=\"openai\",  # NIM models are using OpenAI API\n    base_url=\"https://integrate.api.nvidia.com/v1\",\n    model=\"meta/llama-3.1-8b-instruct\",\n)\nprompt = get_prompt('generic/default') # (2)!\n\nprompts = [prompt.fill({'question': \"What's 2 + 2?\"})]\n\nprint(prompts[0]) # (3)!\noutputs = llm.generate(prompts=prompts)\nprint(outputs[0][\"generation\"]) # (4)!\n</code></pre> <ol> <li> <p>Don't forget to define <code>NVIDIA_API_KEY</code>.</p> <p>To use OpenAI models, use <code>OPENAI_API_KEY</code> and set <code>base_url=https://api.openai.com/v1</code>.</p> </li> <li> <p>Here we use generic/default config.      Note that with API models we can't add special tokens, so prompt template is not specified.</p> <p>See nemo_skills/prompt for more config/template options  or create your own prompts</p> </li> <li> <p>This should print</p> <pre><code>&gt;&gt;&gt; print(prompts[0])\n[{'role': 'system', 'content': ''}, {'role': 'user', 'content': \"What's 2 + 2?\"}]\n</code></pre> <p>If you don't want to use our prompt class, just create this list yourself</p> </li> <li> <p>This should print      <pre><code>&gt;&gt;&gt; print(outputs[0][\"generation\"])\n2 + 2 = 4.\n</code></pre></p> </li> </ol> <pre><code>from nemo_skills.code_execution.sandbox import get_sandbox\nfrom nemo_skills.inference.server.code_execution_model import get_code_execution_model\nfrom nemo_skills.prompt.utils import get_prompt\n\nsandbox = get_sandbox()  # localhost by default\nllm = get_code_execution_model(server_type=\"vllm\", sandbox=sandbox)\nprompt = get_prompt('generic/default', 'llama3-instruct') # (1)!\nprompt.config.system = ( # (2)!\n    \"Environment: ipython\\n\\n\"\n    \"Use Python to solve this math problem.\"\n)\nprompts = [prompt.fill({'question': \"What's 2 + 2?\"})]\nprint(prompts[0]) # (3)!\ncode_tokens = {\n    \"code_begin\": prompt.config.template.code_begin,\n    \"code_end\": prompt.config.template.code_end,\n    \"code_output_begin\": prompt.config.template.code_output_begin,\n    \"code_output_end\": prompt.config.template.code_output_end,\n}\noutputs = llm.generate(prompts=prompts, **code_tokens)\nprint(outputs[0][\"generation\"]) # (4)!\n</code></pre> <ol> <li> <p>Here we use generic/default config      and llama3-instruct template.</p> <p>Note how we are updating system message on the next line (you can also include it in the config directly).</p> <p>See nemo_skills/prompt for more config/template options  or create your own prompts</p> </li> <li> <p>8B model doesn't always follow these instructions, so using 70B or 405B for code execution is recommended.</p> </li> <li> <p>This should print</p> <pre><code>&gt;&gt;&gt; print(prompts[0])\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nEnvironment: ipython\n\nUse Python to solve this math problem.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nWhat's 2 + 2?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <p>If you don't want to use our prompt class, just create this string yourself</p> </li> <li> <p>This should print      <pre><code>&gt;&gt;&gt; print(outputs[0][\"generation\"])\n&lt;|python_tag|&gt;print(2 + 2)&lt;|eom_id|&gt;&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt;\n\ncompleted\n[stdout]\n4\n[/stdout]&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\nThe answer is 4.\n</code></pre></p> <p>The \"4\" in the stdout is coming directly from Python interpreter running in the sandbox.</p> </li> </ol> <p>Note that for self-hosted models we are explicitly adding all the special tokens before sending prompt to an LLM. This is necessary to retain flexibility. E.g. this way we can use base model format with instruct models that we found to work better with few-shot examples.</p> <p>You can learn more about how our prompt formatting works in the prompt format docs.</p> <p>Note</p> <p>You can also use slurm config when launching a server. If you do that, add <code>host=&lt;slurm node hostname&gt;</code> to the <code>get_model/sandbox</code> calls and define <code>NEMO_SKILLS_SSH_KEY_PATH</code> and <code>NEMO_SKILLS_SSH_SERVER</code> env vars to set the connection through ssh.</p>"},{"location":"basics/prerequisites/","title":"Prerequisites","text":""},{"location":"basics/prerequisites/#installation","title":"Installation","text":"<p>To get started first install the repo (python 3.10+). Either clone and run <code>pip install -e .</code> or install directly with</p> <pre><code>pip install git+https://github.com/Kipok/NeMo-Skills.git\n</code></pre>"},{"location":"basics/prerequisites/#environment-variables","title":"Environment variables","text":"<p>Depending on which pipelines you run, you might need to define the following environment variables</p> <pre><code># only needed for training (can opt-out with --disable_wandb)\nexport WANDB_API_KEY=...\n# only needed if using gated models, like llama3.1\nexport HF_TOKEN=...\n# only needed if running inference with OpenAI models\nexport OPENAI_API_KEY=...\n# only needed if running inference with Nvidia NIM models\nexport NVIDIA_API_KEY=...\n</code></pre>"},{"location":"basics/prerequisites/#preparing-data","title":"Preparing data","text":"<p>If you want to run evaluation or use training datasets of popular benchmarks (e.g. math/gsm8k) for data augmentation, you need to run the following commands to prepare the data.</p> <pre><code>python -m nemo_skills.dataset.prepare\n</code></pre> <p>If you're only interested in a subset of datasets (e.g. only math-related or code-related), run with <code>--dataset_groups ...</code> and if you only need a couple of specific datasets, list them directly e.g.</p> <pre><code>python -m nemo_skills.dataset.prepare gsm8k human-eval mmlu ifeval\n</code></pre> <p>If you have the repo cloned locally, the data files will be available inside <code>nemo_skills/dataset/&lt;benchmark&gt;/&lt;split&gt;.jsonl</code> and if you installed from pip, they will be downloaded to wherever the repo is installed, which you can figure out by running</p> <pre><code>python -c \"import nemo_skills; print(nemo_skills.__path__)\"\n</code></pre>"},{"location":"basics/prerequisites/#cluster-configs","title":"Cluster configs","text":"<p>All of the pipeline scripts accept <code>--cluster</code> argument which you can use to control where the job gets executed. That argument picks up one of the configs inside your local cluster_configs folder by default, but you can specify another location with <code>--config_dir</code> or set it in <code>NEMO_SKILLS_CONFIG_DIR</code> env variable. You can also use <code>NEMO_SKILLS_CONFIG</code> env variable instead of the <code>--cluster</code> parameter. The cluster config defines an executor (local or slurm), mounts for data/model access and (slurm-only) various parameters such as account, partition, ssh-tunnel arguments and so on.</p> <p>We use NeMo-Run for managing our experiments with local and slurm-based execution supported (please open an issue if you need to run our code on other kinds of clusters). This means that even if you need to submit jobs on slurm, you do it from your local machine by defining an appropriate cluster config and nemo-run will package and upload your code, data and manage all complexities of slurm scheduling. Check their documentation to learn how to fetch logs, check status, cancel jobs, etc.</p> <p>Note</p> <p>NeMo-Run will only package the code tracked by git (as well as all jsonl files from <code>nemo_skills/dataset</code>). Any non-tracked files will not be automatically available inside the container or uploaded to slurm.</p> <p>We use Hydra for most of the scripts, so it's a good idea to read through their documentation if that's the first time you see it.</p> <p>Most of our pipeline scripts use a mix of normal command-line arguments and Hydra style config overrides (usually formatted as <code>++arg_name</code>). Whenever you see this, it means that the regular <code>--arg_name</code> parameters are used to control the wrapper script itself and all other parameters are directly passed into the underlying <code>nemo_skills/...</code> script called by the wrapper.</p>"},{"location":"basics/prerequisites/#running-pipelines","title":"Running pipelines","text":"<p>All of the pipeline scripts can be called in 3 equivalent ways. As an example let's see how to run evaluation on 10 samples from gsm8k and math benchmarks</p> ns command-line entrypoint<pre><code>ns eval \\\n    --cluster=local \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-8b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1 \\\n    --benchmarks=gsm8k:0,math:0 \\\n    --output_dir=/workspace/test-eval \\\n    ++max_samples=10\n</code></pre> calling python module directly<pre><code>python -m nemo_skills.pipeline.eval \\\n    --cluster=local \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-8b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1 \\\n    --benchmarks=gsm8k:0,math:0 \\\n    --output_dir=/workspace/test-eval \\\n    ++max_samples=10\n</code></pre> using python api<pre><code>from nemo_skills.pipeline import wrap_arguments\nfrom nemo_skills.pipeline.cli import eval\n\neval(\n    cluster=\"local\",\n    server_type=\"openai\",\n    model=\"meta/llama-3.1-8b-instruct\",\n    server_address=\"https://integrate.api.nvidia.com/v1\",\n    benchmarks=\"gsm8k:0,math:0\",\n    output_dir=\"/workspace/test-eval\",\n    # arguments of the underlying script need to be wrapped\n    # you can separate multiple arguments with space or newline\n    ctx=wrap_arguments(\"++max_samples=10\"),\n)\n</code></pre> <p>You can also chain multiple pipelines together to set proper slurm dependencies using <code>--run_after</code> parameter. See an example in training documentation.</p>"},{"location":"basics/prerequisites/#local-execution","title":"Local execution","text":"<p>To run scripts locally we use docker containers, so make sure you have NVIDIA Container Toolkit set up on your machine.</p> <p>All of our scripts assume that data or models are mounted inside the appropriate container so before running any commands make sure to modify cluster_configs/example-local.yaml. It's convenient to rename it to local.yaml (so you can use <code>--cluster local</code>) after you defined necessary mounts.</p> <p>Most of our containers are quite heavy, so the first time you run a job that requires a large container, it will take a while to pull it. You can manually run <code>docker pull &lt;container&gt;</code> for all containers defined in the local config to cache them.</p>"},{"location":"basics/prerequisites/#slurm-jobs","title":"Slurm jobs","text":"<p>If you're running on slurm, you need to define some additional information inside cluster config.</p> <p>Populate the commented out fields inside cluster_configs/example-slurm.yaml. It's convenient to rename it to slurm.yaml (so you can use <code>--cluster slurm</code>) or a cluster name if you use multiple slurm clusters.</p>"},{"location":"basics/prompt-format/","title":"Prompt utilities","text":"<p>Note</p> <p>While some of the sections below mention multi-turn prompts, we don't actually support them at the moment. This is mainly because we don't have a real use-case for multi-turn conversations in our work. Please open an issue if you need to use multi-turn prompts.</p> <p>Our prompts are configured via two input yaml files: prompt template and prompt config.</p>"},{"location":"basics/prompt-format/#prompt-template","title":"Prompt template","text":"<p>The template file defines model-specific special tokens, e.g. bos, turn tokens, user/assistant/system message, special tokens for code execution, etc. All of the templates that we support by default are available in nemo_skills/prompt/template folder. Here is an example template for llama3-instruct models:</p> <pre><code># Prompt specification for the original Llama3-instruct model\n\n# these tokens are always used to construct a prompt like this\n#\n#   single-turn:\n#     &lt;text_begin&gt;&lt;system_begin&gt;{system}&lt;system_end&gt;&lt;user_begin&gt;{user}&lt;user_end&gt;&lt;assistant_begin&gt;{generation}\n#   multi-turn:\n#     &lt;text_begin&gt;&lt;system_begin&gt;{system}&lt;system_end&gt;&lt;user_begin&gt;{user1}&lt;user_end&gt;&lt;assistant_begin&gt;{assistant1}&lt;assistant_end&gt;...\n#     &lt;user_begin&gt;{userN}&lt;user_end&gt;&lt;assistant_begin&gt;{generation}\n\ntext_begin: \"&lt;|begin_of_text|&gt;\"\n\nsystem_begin: \"&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n\"\nsystem_end: \"&lt;|eot_id|&gt;\"\n\nuser_begin: \"&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n\"\nuser_end: \"&lt;|eot_id|&gt;\"\n\nassistant_begin: \"&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"\nassistant_end: \"&lt;|eot_id|&gt;\"\n\nstop_phrases: [\"&lt;|eot_id|&gt;\"]\n\n# used to execute code within these tags\ncode_begin: '&lt;|python_tag|&gt;'\ncode_end: '&lt;|eom_id|&gt;'\n# used to extract the code output\ncode_output_begin: '&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt;'\ncode_output_end: '&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;'\n</code></pre> <p>You can specify a particular template with <code>++prompt_template=...</code>. If you don't add a .yaml extension (e.g. <code>++prompt_template=llama3-instruct</code>), we assume you want to use one of the existing templates and will search in the included folder. If you provide a full path, we will take the file you specify instead.</p> <p>Note</p> <p>If you're using OpenAI server type (models are hosted elsewhere), you cannot provide the template as we cannot add any special tokens and have to send the user/assistant messages following the OpenAI API. For all self-hosted models, the template is required.</p>"},{"location":"basics/prompt-format/#prompt-config","title":"Prompt config","text":"<p>The prompt config contains user and system messages with placeholders for keys from a data file. The configs are model independent (any model can be used with any config). All of the configs that we support by default are available in nemo_skills/prompt/config folder. Here is an example prompt for math evaluations:</p> <pre><code># default prompt for all math benchmarks (e.g. gsm8k, math)\n\nfew_shot_examples:\n  prefix: \"Here are some examples of problems and solutions you can refer to.\\n\\n\"\n  template: \"Problem:\\n{problem}\\n\\nSolution:\\n{solution}\\n\\n\\n\\n\\n\\n\"\n  suffix: \"Here is the problem you need to solve:\\n\"\n  # this is built as &lt;prefix&gt;{template.format(example1)}{template.format(example2)}...{template.format(exampleN)}&lt;suffix&gt;\n  # and available as {examples} key in the final prompt\n  # if examples_type is not specified, then {examples} will be empty\n  # by default there are no examples, but can be changed from code/cmd\n\nsystem: \"\"\n\nuser: |-\n  Solve the following math problem. Make sure to put the answer (and only answer) inside \\boxed{{}}.\n\n  {examples}{problem}\n</code></pre> <p>Note that we use <code>{problem}</code>, <code>{solution}</code> and <code>{examples}</code> format strings here. The <code>{examples}</code> is a special key that will be used to include few shot examples you specify above (it's empty unless you add <code>++examples_type</code> or specify it in the config like e.g. in llama3-gsm8k prompt). All other keys will need to be specified when you call <code>prompt.fill</code> (more on that in the prompt-api section) so that we can replace placeholders with actual input.</p> <p>The input for few shot examples always comes from one of the available example types in here. E.g. in the llama3-gnstruct/gsm8k prompt the <code>gsm8k_standard_few_shot</code> examples from here are used.</p>"},{"location":"basics/prompt-format/#prompt-api","title":"Prompt API","text":"<p>If you're running one of the pipeline scripts, you can control the prompt by using</p> <pre><code>++prompt_template=...\n++prompt_config=...\n++examples_type=...\n</code></pre> <p>If you're implementing a new script, you can use the following code to create a prompt and then use it</p> <pre><code>from nemo_skills.prompt.utils import get_prompt\n\nprompt = get_prompt('generic/math', 'llama3-instruct')\nprint(prompt.fill({'problem': \"What's 2 + 2?\"}))\n</code></pre> <p>which outputs</p> <pre><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nSolve the following math problem. Make sure to put the answer (and only answer) inside \\boxed{}.\n\nWhat's 2 + 2?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <p>Or if you want to skip the template and use OpenAI API</p> <pre><code>from nemo_skills.prompt.utils import get_prompt\n\nprompt = get_prompt('generic/math')\nprint(prompt.fill({'problem': \"What's 2 + 2?\"}))\n</code></pre> <p>which outputs</p> <pre><code>[\n  {\n    'role': 'system',\n    'content': ''\n  },\n  {\n    'role': 'user',\n    'content': \"Solve the following math problem. Make sure to put the answer (and only answer) inside \\\\boxed{}.\\n\\nWhat's 2 + 2?\"\n  }\n]\n</code></pre> <p>You can also have a look at the tests to see more examples of using our prompt API.</p>"},{"location":"code-execution/sandbox/","title":"Sandbox for code execution","text":"<p>Our pipeline relies on Python interpreter to execute code generated by LLMs. This creates a security risk, since we are executing arbitrary code that we do not have full control over. To partially address this, we provide a basic sandbox that we use to execute code and validate the correctness of LLM-generated answers.</p>"},{"location":"code-execution/sandbox/#local-sandbox","title":"Local sandbox","text":"<p>The default sandbox option used in our pipeline is a local docker container. Check out nemo_skills/code_execution/local_sandbox for implementation details.</p> <p>Please note that our provided sandbox is not fully secure and you are strongly encouraged to setup a properly configured virtual machine such that generated code executes in an unprivileged environment with no external network access unless necessary.</p>"},{"location":"code-execution/sandbox/#piston-sandbox","title":"Piston sandbox","text":"<p>A better alternative is to host a Piston server in a properly configured VM. If you're using a Piston server (you need to host it yourself), add the following parameters to the relevant scripts</p> <pre><code>++sandbox_type=piston\n++sandbox.host=&lt;where your server is hosted, e.g. https://emkc.org/api/v2/piston&gt;\n</code></pre>"},{"location":"code-execution/sandbox/#other-sandboxes","title":"Other sandboxes","text":"<p>Our sandbox API makes no assumptions on where or how the code is executed, so it's very easy to extend it. E.g. you can use AWS Lambda functions or other similar offerings. Please open an issue if you'd like us to add support for another sandbox in the future.</p>"},{"location":"openmathinstruct2/","title":"How to reproduce our results","text":"<p>This section has instructions for reproducing OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data.</p> <p>Please note that unless you have an access to a large GPU cluster, it might take a very long time for some of the commands to complete!</p> <ul> <li>Model evaluation</li> <li>Dataset construction</li> <li>Model training</li> </ul> <p>Note</p> <p>If you want to reproduce results for OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset please check out v0.1.1 branch of the NeMo-Skills repo and read the instructions in there.</p>"},{"location":"openmathinstruct2/dataset/","title":"Dataset construction","text":"<p>Here are the commands you can run to re-create OpenMathInstruct-2 dataset. We assume you have <code>/workspace</code> defined in your cluster config and are running all commands on a slurm cluster. Change the commands accordingly if running locally (but it's going to take a lot of time). We also assume you have the Llama3.1 405B on that cluster inside <code>/trt_models/llama-3.1-405b-instruct</code> (should be mounted in your config) that's been converted to TensorRT-LLM format. See generation docs for how you can change the below commands to instead run inference through Nvidia NIM API.</p>"},{"location":"openmathinstruct2/dataset/#prepare-the-seed-data","title":"Prepare the seed data","text":"<pre><code>python -m nemo_skills.dataset.prepare gsm8k math\n</code></pre>"},{"location":"openmathinstruct2/dataset/#solution-augmentation","title":"Solution augmentation","text":"<p>We generate multiple new solutions for each of the original training set problems.</p> <p>MATH dataset.</p> <pre><code>ns generate \\\n    --cluster=slurm \\\n    --server_type=trtllm \\\n    --model=/trt_models/llama-3.1-405b-instruct \\\n    --server_gpus=8 \\\n    --server_nodes=2 \\\n    --num_random_seeds=512 \\\n    --output_dir=/workspace/solution-augmentation/math \\\n    --eval_args=\"++eval_type=math\" \\\n    ++dataset=math \\\n    ++split=train_full \\\n    ++prompt_config=generic/math-base \\\n    ++examples_type=math_text_detailed \\\n    ++prompt_template=llama3-base\n</code></pre> <p>GSM8K dataset.</p> <pre><code>ns generate \\\n    --cluster=slurm \\\n    --server_type=trtllm \\\n    --model=/trt_models/llama-3.1-405b-instruct \\\n    --server_gpus=8 \\\n    --server_nodes=2 \\\n    --num_random_seeds=64 \\\n    --output_dir=/workspace/solution-augmentation/gsm8k \\\n    --eval_args=\"++eval_type=math\" \\\n    ++dataset=gsm8k \\\n    ++split=train_full \\\n    ++prompt_config=generic/math-base \\\n    ++examples_type=gsm8k_text_detailed \\\n    ++prompt_template=llama3-base\n</code></pre>"},{"location":"openmathinstruct2/dataset/#problem-augmentation","title":"Problem augmentation","text":"<p>We generate new problems using the problems from the training sets as a \"seed\".</p> <p>MATH dataset.</p> <pre><code>ns generate \\\n    --cluster=slurm \\\n    --server_type=trtllm \\\n    --model=/trt_models/llama-3.1-405b-instruct \\\n    --server_gpus=8 \\\n    --server_nodes=2 \\\n    --num_random_seeds=80 \\\n    --output_dir=/workspace/problem-augmentation/math \\\n    ++dataset=math \\\n    ++split=train_full \\\n    ++prompt_config=generic/problem-augmentation \\\n    ++examples_type=math_problem_augmentation \\\n    ++prompt_template=llama3-instruct \\\n    ++generation_key=problem\n</code></pre> <p>GSM8K dataset.</p> <pre><code>ns generate \\\n    --cluster=slurm \\\n    --server_type=trtllm \\\n    --model=/trt_models/llama-3.1-405b-instruct \\\n    --server_gpus=8 \\\n    --server_nodes=2 \\\n    --num_random_seeds=10 \\\n    --output_dir=/workspace/problem-augmentation/gsm8k \\\n    ++dataset=gsm8k \\\n    ++split=train_full \\\n    ++prompt_config=generic/problem-augmentation-similar \\\n    ++examples_type=gsm8k_problem_augmentation \\\n    ++prompt_template=llama3-instruct \\\n    ++generation_key=problem\n</code></pre>"},{"location":"openmathinstruct2/dataset/#solutions-for-augmented-data","title":"Solutions for augmented data","text":"<p>Solution augmentation for the newly generated problems. We generate 32 solutions for each of the new problems.</p> <p>We use the Python API in commands below.</p> <p>MATH dataset.</p> <pre><code>from nemo_skills.pipeline import wrap_arguments\nfrom nemo_skills.pipeline.cli import generate\n\n# we generated 80 new problems from each original seed problem, so we have a loop\n# to now generate 32 solutions for each of those 80 new data files\nfor i in range(80):\n    generate(\n        cluster=\"slurm\",\n        server_type=\"trtllm\",\n        model=\"/trt_models/llama-3.1-405b-instruct\",\n        server_gpus=8,\n        server_nodes=2,\n        num_random_seeds=32,\n        output_dir=f\"/workspace/new-problems-solution-augmentation/math/problem-set{i}\",\n        ctx=wrap_arguments(\n            f\"++input_file=/workspace/solution-augmentation/math/generation/output-rs{i} \"\n            f\"++prompt_config=generic/math-base \"\n            f\"++examples_type=math_text_detailed \"\n            f\"++prompt_template=llama3-base \"\n        ),\n    )\n</code></pre> <p>GSM8K dataset.</p> <pre><code>from nemo_skills.pipeline import wrap_arguments\nfrom nemo_skills.pipeline.cli import generate\n\n# we generated 10 new problems from each original seed problem, so we have a loop\n# to now generate 32 solutions for each of those 10 new data files\nfor i in range(10):\n    generate(\n        cluster=\"slurm\",\n        server_type=\"trtllm\",\n        model=\"/trt_models/llama-3.1-405b-instruct\",\n        server_gpus=8,\n        server_nodes=2,\n        num_random_seeds=32,\n        output_dir=f\"/workspace/new-problems-solution-augmentation/gsm8k/problem-set{i}\",\n        ctx=wrap_arguments(\n            f\"++input_file=/workspace/solution-augmentation/gsm8k/generation/output-rs{i} \"\n            f\"++prompt_config=generic/math-base \"\n            f\"++examples_type=gsm8k_text_detailed \"\n            f\"++prompt_template=llama3-base \"\n        ),\n    )\n</code></pre> <p>Add majority answer as the ground-truth answer. Either copy the data locally or run this command on a slurm node. You also need to specify the full path to where <code>/workspace</code> is mounted (we will make it more convenient in the near future by providing the same Python/cmdline API as for other scripts).</p> <pre><code>import subprocess\n\n# for MATH\ndata_folder = \"&lt;path to where /workspace is&gt;/new-problems-solution-augmentation/math\"\nfor i in range(80):\n    cmd = (\n        f'python -m nemo_skills.evaluation.fill_majority_answer '\n        f'    ++input_files=\"{data_folder}/problem-set{i}/generation/output-rs*.jsonl\" '\n    )\n    subprocess.run(cmd, shell=True, check=True)\n\n# for GSM8K\ndata_folder = \"&lt;path to where /workspace is&gt;/new-problems-solution-augmentation/gsm8k\"\nfor i in range(10):\n    cmd = (\n        f'python -m nemo_skills.evaluation.fill_majority_answer '\n        f'    ++input_files=\"{data_folder}/problem-set{i}/generation/output-rs*.jsonl\" '\n    )\n    subprocess.run(cmd, shell=True, check=True)\n</code></pre>"},{"location":"openmathinstruct2/dataset/#decontamination","title":"Decontamination","text":"<p>We test against GSM8K, MATH, AMC 2023, and AIME 2024.</p> <p>Retrieve top-5 similar items from the test sets <pre><code>python -m nemo_skills.inference.retrieve_similar \\\n    ++retrieve_from=\"./nemo_skills/dataset/gsm8k/test.jsonl ./nemo_skills/dataset/math/test.jsonl ./nemo_skills/dataset/amc23/test.jsonl ./nemo_skills/dataset/aime24/test.jsonl\" \\\n    ++compare_to=\"&lt;path to workspace&gt;/new-problems-solution-augmentation/**/output-rs0.jsonl\" \\\n    ++output_file=&lt;path to workspace&gt;/new-problems-solution-augmentation/contamination-retrieved.jsonl \\\n    ++top_k=5\n</code></pre></p> <p>Note</p> <p>Currently the above command doesn't run inside docker, so you will need to install additional packages.</p> <p>Next, you need to run LLM inference to check those closest found problems from the output file. We use the Llama3.1-405B-Instruct model for this, and here's one way of doing it via Nvidia API catalog.</p> <pre><code>ns check_contamination \\\n    --cluster=local \\\n    --input_file=/workspace/new-problems-solution-augmentation/contamination-retrieved.jsonl \\\n    --output_file=/workspace/new-problems-solution-augmentation/contamination-llm.jsonl \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-405b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1 \\\n    ++check_both_ways=True\n</code></pre> <p>Identify all the problems for which the <code>contaminated</code> key has the output True. Add the entry <code>\"contaminated\": True</code> in all the generation files in <code>&lt;path to workspace&gt;/new-problems-solution-augmentation/</code>. Here is a sample python script for this:</p> <pre><code>def load_contaminated_problems(jsonl_file):\n    contaminated_problems = set()\n    with open(jsonl_file, 'r') as f:\n        for line in f:\n            data = json.loads(line)\n            if data['contaminated']:\n                contaminated_problems.add(data['problem'])\n    return contaminated_problems\n\ndef update_output_files(directory, contaminated_problems):\n    file_pattern = str(Path(directory) / '**' / 'output-rs*.jsonl')\n    for file_path in glob.glob(file_pattern, recursive=True):\n        temp_file_path = Path(file_path).with_suffix('.temp')\n\n        with open(file_path, 'r') as input_file, open(temp_file_path, 'w') as output_file:\n            for line in input_file:\n                data = json.loads(line)\n                if data['problem'] in contaminated_problems:\n                    data['contaminated'] = True\n                json.dump(data, output_file)\n                output_file.write('\\n')\n\n        # Replace the original file with the updated one\n        temp_file_path.replace(file_path)\n        print(f\"Updated file: {file_path}\")\n\ncontaminated_problems = load_contaminated_problems(\"&lt;path to workspace&gt;/new-problems-solution-augmentation/contamination-llm.jsonl\")\n\nupdate_output_files(\"&lt;path to workspace&gt;/new-problems-solution-augmentation/\", contaminated_problems)\n</code></pre>"},{"location":"openmathinstruct2/dataset/#converting-to-sft-format","title":"Converting to SFT format","text":"<p>Now all the data is generated and you can follow up by converting it to the SFT format. We remove the problems marked as contaminated. We also remove solutions with length &gt; 1024 Llama tokens. To avoid the models from generating extremely short solutions, we remove solutions shorter than 200 characters.</p> <pre><code>python -m nemo_skills.training.prepare_sft_data \\\n    ++prompt_template=llama3-instruct \\\n    ++prompt_config=generic/math \\\n    ++input_files=\"&lt;path to workspace&gt;/solution-augmentation/**/output-rs*.jsonl &lt;path to workspace&gt;/new-problems-solution-augmentation/**/output-rs*.jsonl\" \\\n    ++output_path=&lt;path to workspace&gt;/sft_data.jsonl \\\n    ++filters.remove_contamindated=true \\\n    ++filters.remove_len_outlier_solutions=true \\\n    ++use_chars_for_min_length=true \\\n    ++min_solution_length=200 \\\n    ++hf_model_name=\"meta-llama/Meta-Llama-3.1-8B\" \\\n    ++max_solution_length=1024 \\\n    ++generation_suffix='\"&lt;|eot_id|&gt;\"'\n</code></pre>"},{"location":"openmathinstruct2/dataset/#dataset-contamination-explorer","title":"Dataset contamination explorer","text":"<p>To reproduce our dataset contamination explorer demo refer to dataset_explorer_demo/README.md</p>"},{"location":"openmathinstruct2/evaluation/","title":"Model evaluation","text":"<p>Here are the commands you can run to reproduce our evaluation numbers. The commands below are for OpenMath-2-Llama3.1-8b model as an example. We assume you have <code>/workspace</code> defined in your cluster config and are executing all commands from that folder locally. Change all commands accordingly if running on slurm or using different paths.</p>"},{"location":"openmathinstruct2/evaluation/#download-models","title":"Download models","text":"<p>Get the model from HF. E.g.</p> <pre><code>pip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download nvidia/OpenMath2-Llama3.1-8B --local-dir OpenMath2-Llama3.1-8B\n</code></pre>"},{"location":"openmathinstruct2/evaluation/#convert-to-tensorrt-llm","title":"Convert to TensorRT-LLM","text":"<p>Convert the model to TensorRT-LLM format. This is optional, but highly recommended for more exact results and faster inference. If you skip it, replace <code>--server_type trtllm</code> with <code>--server-type vllm</code> in the commands below and change model path to <code>/workspace/OpenMath2-Llama3.1-8B</code>. You might also need to set smaller batch size for vllm.</p> <pre><code>ns convert \\\n    --cluster=local \\\n    --input_model=/workspace/OpenMath2-Llama3.1-8B \\\n    --output_model=/workspace/openmath2-llama3.1-8b-trtllm \\\n    --convert_from=hf \\\n    --convert_to=trtllm \\\n    --num_gpus=1 \\\n    --hf_model_name=nvidia/OpenMath2-Llama3.1-8B\n</code></pre> <p>Change the number of GPUs if you have more than 1 (required for 70B model).</p>"},{"location":"openmathinstruct2/evaluation/#prepare-evaluation-data","title":"Prepare evaluation data","text":"<pre><code>python -m nemo_skills.dataset.prepare gsm8k math amc23 aime24 omni-math\n</code></pre>"},{"location":"openmathinstruct2/evaluation/#run-greedy-decoding","title":"Run greedy decoding","text":"<pre><code>ns eval \\\n    --cluster=local \\\n    --model=/workspace/openmath2-llama3.1-8b-trtllm \\\n    --server_type=trtllm \\\n    --output_dir=/workspace/openmath2-llama3.1-8b-eval \\\n    --benchmarks=aime24:0,amc23:0,math:0,gsm8k:0,omni-math:0 \\\n    --server_gpus=1 \\\n    --num_jobs=1 \\\n    ++prompt_template=llama3-instruct \\\n    ++batch_size=512 \\\n    ++inference.tokens_to_generate=4096\n</code></pre> <p>If running on slurm, you can set <code>--num_jobs</code> to a bigger number of -1 to run each benchmark in a separate node. The number of GPUs need to match what you used in the conversion command.</p> <p>After the generation is done, we want to run LLM-as-a-judge evaluation to get more accurate numbers than symbolic comparison. You need to define <code>OPENAI_API_KEY</code> for the command below to work.</p> <pre><code>ns llm_math_judge \\\n    --cluster=local \\\n    --model=gpt-4o \\\n    --server_type=openai \\\n    --server_address=https://api.openai.com/v1 \\\n    --input_files=\"/workspace/openmath2-llama3.1-8b-eval/eval-results/**/output*.jsonl\"\n</code></pre> <p>Finally, to print the metrics run</p> <pre><code>ns summarize_results /workspace/openmath2-llama3.1-8b-eval/eval-results --cluster local\n</code></pre> <p>This should print the metrics including both symbolic and judge evaluation. The judge is typically more accurate.</p> <pre><code>------------------------------------------------- aime24 ------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 30          | 10.00            | 10.00         | 10.00        | 10.00       | 6.67\n\n\n------------------------------------------------- gsm8k -------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 1319        | 90.75            | 91.70         | 90.75        | 91.70       | 0.00\n\n\n----------------------------------------------- omni-math -----------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 4428        | 18.97            | 22.22         | 18.11        | 23.08       | 2.55\n\n\n-------------------------------------------------- math -------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 5000        | 67.70            | 68.10         | 67.50        | 68.30       | 1.36\n\n\n------------------------------------------------- amc23 -------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 40          | 32.50            | 40.00         | 32.50        | 40.00       | 0.00\n</code></pre> <p>The numbers may vary by 1-2% depending on the server type, number of GPUs and batch size used.</p>"},{"location":"openmathinstruct2/evaluation/#run-majority-voting","title":"Run majority voting","text":"<pre><code>ns eval \\\n    --cluster=local \\\n    --model=/workspace/openmath2-llama3.1-8b-trtllm \\\n    --server_type=trtllm \\\n    --output_dir=/workspace/openmath2-llama3.1-8b-eval \\\n    --benchmarks=aime24:256,amc23:256,math:256,gsm8k:256,omni-math:256 \\\n    --server_gpus=1 \\\n    --num_jobs=1 \\\n    --skip_greedy \\\n    ++prompt_template=llama3-instruct \\\n    ++batch_size=512 \\\n    ++inference.tokens_to_generate=4096\n</code></pre> <p>This will take a very long time unless you run on slurm cluster. After the generation is done, you will be able to see symbolic scores right away. You can evaluate with the judge by first creating new files with majority answers. E.g. for \"math\" benchmark run</p> <pre><code>python -m nemo_skills.evaluation.fill_majority_answer \\\n    ++input_files=\"./openmath2-llama3.1-8b-eval/eval-results/math/output-rs*.jsonl\" \\\n    ++fill_key=predicted_answer\n</code></pre> <p>This will replace <code>predicted_answer</code> in all files with majority answer.</p> <p>After that, let's copy just a single of those files into a new folder so that we can run the llm-judge pipeline on them.</p> <pre><code>mkdir -p ./openmath2-llama3.1-8b-eval/eval-results-majority/math\ncp ./openmath2-llama3.1-8b-eval/eval-results/math/output-rs0.jsonl ./openmath2-llama3.1-8b-eval/eval-results-majority/math/\n</code></pre> <p>Repeat the above steps for all benchmarks. Now we are ready to run the judge pipeline and summarize results after it is finished. You need to define <code>OPENAI_API_KEY</code> for the command below to work.</p> <pre><code>ns llm_math_judge \\\n    --cluster=local \\\n    --model=gpt-4o \\\n    --server_type=openai \\\n    --server_address=https://api.openai.com/v1 \\\n    --input_files=\"/workspace/openmath2-llama3.1-8b-eval/eval-results-majority/**/output*.jsonl\"\n</code></pre> <pre><code>ns summarize_results /workspace/openmath2-llama3.1-8b-eval/eval-results-majority --cluster local\n</code></pre> <p>This will print majority results (they will be labeled as <code>majority@1</code> since we fused them into a single file). You can also ignore the symbolic score as it's not accurate anymore after we filled majority answers.</p>"},{"location":"openmathinstruct2/training/","title":"Model training","text":"<p>We assume you have <code>/workspace</code> defined in your cluster config and are executing all commands from that folder locally. Change all commands accordingly if running on slurm or using different paths.</p>"},{"location":"openmathinstruct2/training/#download-data","title":"Download data","text":"<p>Get the data from HuggingFace. This might take 20-30 minutes (or more depending on your network connection) and will use ~20Gb of RAM.</p> <pre><code>import json\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ndataset = load_dataset('nvidia/OpenMathInstruct-2', split='train')\n\nprint(\"Converting dataset to jsonl format\")\noutput_file = \"openmathinstruct2.jsonl\"\nwith open(output_file, 'w', encoding='utf-8') as f:\n    for item in tqdm(dataset):\n        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n\nprint(f\"Conversion complete. Output saved as {output_file}\")\n</code></pre> <p>You can also download a subset of the data by using e.g. <code>split='train_5M'</code> that we used to train 70B model. See the dataset page for more details about this.</p>"},{"location":"openmathinstruct2/training/#convert-to-sft-format","title":"Convert to SFT format","text":"<p>Convert the data into the SFT format that NeMo-Aligner understands.</p> <pre><code>python -m nemo_skills.training.prepare_sft_data \\\n    ++prompt_template=llama3-instruct \\\n    ++prompt_config=generic/math \\\n    ++preprocessed_dataset_files=&lt;path to workspace&gt;/openmathinstruct2.jsonl \\\n    ++output_key=generated_solution \\\n    ++output_path=&lt;path to workspace&gt;/openmathinstruct2-sft.jsonl \\\n    ++filters.drop_multi_boxed=false \\\n    ++filters.trim_prefix=false \\\n    ++filters.trim_solutions=false \\\n    ++filters.drop_incorrect_arithmetic=false \\\n    ++filters.split_arithmetic=false \\\n    ++generation_suffix='\"&lt;|eot_id|&gt;\"';\n</code></pre>"},{"location":"openmathinstruct2/training/#prepare-base-model","title":"Prepare base model","text":"<p>Download the base model and convert it to NeMo format. The instructions below are for Llama3.1-8B, but the same commands should work for 70B model as well.</p> <pre><code>pip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download meta-llama/Llama-3.1-8B --local-dir Llama-3.1-8B\n\nns convert \\\n    --cluster=local \\\n    --input_model=/workspace/Llama-3.1-8B \\\n    --output_model=/workspace/llama3.1-8b-nemo \\\n    --convert_from=hf \\\n    --convert_to=nemo \\\n    --num_gpus=1 \\\n    --hf_model_name=meta-llama/Llama-3.1-8B\n</code></pre>"},{"location":"openmathinstruct2/training/#run-training","title":"Run training","text":"<p>Run the training (assuming slurm configuration here with the same folder structure). If your cluster has strict timeout policy, you can run multiple dependent jobs with <code>--num_training_jobs=N</code>.</p> <pre><code>ns train \\\n    --cluster=slurm \\\n    --expname=openmathinstruct2-repro-8b \\\n    --output_dir=/workspace/openmathinstruct2-repro/checkpoints \\\n    --nemo_model=/workspace/llama3.1-8b-nemo \\\n    --num_nodes=8 \\\n    --num_gpus=8 \\\n    --average_steps 10000 20000 30000 40000 50000 60000 \\\n    --training_data=/workspace/openmathinstruct2-sft.jsonl \\\n    ++model.data.train_ds.micro_batch_size=4 \\\n    ++model.tensor_model_parallel_size=4 \\\n    ++model.pipeline_model_parallel_size=1 \\\n    ++model.optim.lr=2e-5 \\\n    ++trainer.sft.save_interval=10000 \\\n    ++trainer.sft.max_steps=60000 \\\n    ++trainer.sft.max_epochs=-1\n</code></pre> <p>For 70B model, we used 5M data subset and the following parameters, but training it longer is likely going to improve results.</p> <pre><code>ns train \\\n    --cluster=slurm \\\n    --expname=openmathinstruct2-repro-70b \\\n    --output_dir=/workspace/openmathinstruct2-repro-70b/checkpoints \\\n    --nemo_model=/workspace/llama3.1-70b-nemo \\\n    --num_nodes=32 \\\n    --num_gpus=8 \\\n    --average_steps 3330 6660 9990 13320 16650 20000 \\\n    --training_data=/workspace/openmathinstruct2-sft-5M.jsonl \\\n    ++model.data.train_ds.micro_batch_size=1 \\\n    ++model.tensor_model_parallel_size=8 \\\n    ++model.pipeline_model_parallel_size=2 \\\n    ++model.optim.lr=1e-5 \\\n    ++trainer.sft.save_interval=3330 \\\n    ++trainer.sft.max_steps=20000 \\\n    ++trainer.sft.max_epochs=-1\n</code></pre> <p>If you have a job timeout, it's necessary to set the maximum time per run to 40 minutes before the timeout to allow for the final checkpoint to be saved. E.g. if your timeout is 4 hours, add <code>++exp_manager.max_time_per_run=00:03:20:00</code></p> <p>If you want to follow up with checkpoint conversion and evaluation, see training docs for an example of how to do it through a convenient Python API.</p>"},{"location":"pipelines/checkpoint-conversion/","title":"Checkpoint conversion","text":"<p>Info</p> <p>This pipeline starting script is nemo_skills/pipeline/convert.py</p> <p>All extra parameters are passed to one of the following scripts</p> <ul> <li> <p>For conversion to NeMo:</p> <ul> <li>Default: nemo_skills/conversion/hf_to_nemo.py</li> <li>If <code>--model_type=qwen</code>: nemo_skills/conversion/hf_to_nemo_qwen.py</li> </ul> </li> <li> <p>For conversion to TensorRT-LLM:</p> <ul> <li>Default: nemo_skills/conversion/hf_to_trtllm.py</li> <li>If <code>--model_type=qwen</code>: nemo_skills/conversion/hf_to_trtllm_qwen.py</li> </ul> </li> <li> <p>For conversion to HuggingFace: nemo_skills/conversion/nemo_to_hf.py</p> </li> </ul> <p>We support 3 common model formats. Here are some recommendations on when each format should be used.</p> <ul> <li> <p>HuggingFace (via vLLM).   If you want to run a small-scale generation quickly   or play with models, it's most convenient to use HF format directly via a vllm server.</p> </li> <li> <p>TensorRT-LLM.   If you want to run a large-scale generation, it's highly recommended to use TensoRT-LLM format.   The time it takes to convert the checkpoint will be more than offset by a much faster generation   than either vLLM or NeMo.</p> </li> <li> <p>NeMo.   NeMo is the only supported format for training, so you need to use it with the   training pipeline. We don't recommend running inference in NeMo   as it is much slower than both vLLM and TensorRT-LLM servers.</p> </li> </ul> <p>To convert the checkpoint from one format to another use a command like this</p> <pre><code>ns convert \\\n    --cluster=slurm \\\n    --input_model=/hf_models/Meta-Llama-3.1-70B-Instruct \\\n    --output_model=/trt_models/llama3.1-70b-instruct \\\n    --convert_from=hf \\\n    --convert_to=trtllm \\\n    --num_gpus=8 \\\n    --hf_model_name=meta-llama/Meta-Llama-3.1-70B-Instruct\n</code></pre> <p>You can provide any extra arguments that will be passed directly to the underlying conversion scripts. Here are a few things to keep in mind</p> <ul> <li>We currently only support Llama-based and Qwen-based models (enable with <code>--model_type qwen</code>). The other kinds   of models are most likely easy to add, we just didn't have a use-case for them yet (please open an issue if the   model you want to use is not supported).</li> <li>You cannot convert from trtllm format, only to it.</li> <li>You cannot convert from nemo to trtllm directly and need to do it in 2 stages, to nemo-&gt;hf and then hf-&gt;trtllm.</li> <li>Please check NeMo and TensorRT-LLM   documentation to learn best recommended parameters for converting each specific model.</li> </ul>"},{"location":"pipelines/decontamination/","title":"LLM-based data decontamination","text":"<p>Info</p> <p>This pipeline starting script is nemo_skills/pipeline/check_contamination.py</p> <p>All extra parameters are passed to nemo_skills/inference/check_contamination.py</p> <p>We implemented an LLM-based data decontamination pipeline following lmsys methodology.</p> <p>There are two main ways how you can use this pipeline: to check existing dataset for contamination and to decontaminate the training dataset by removing all contaminated questions.</p>"},{"location":"pipelines/decontamination/#to-check-for-contamination","title":"To check for contamination","text":"<p>Let's say you want to check for contamination of MATH training set with MATH, AMC-23 and AIME-24 test sets.</p> <p>First, we need to retrieve top-k similar questions from the training set. Assuming you're running from locally installed repository you can do it in the following way</p> <pre><code>python -m nemo_skills.inference.retrieve_similar \\\n    ++retrieve_from=./nemo_skills/dataset/math/train_full.jsonl \\\n    ++compare_to=\"./nemo_skills/dataset/math/test.jsonl ./nemo_skills/dataset/amc23/test.jsonl ./nemo_skills/dataset/aime24/test.jsonl\" \\\n    ++output_file=./math-contamination-retrieved.jsonl \\\n    ++top_k=1\n</code></pre> <p>Note</p> <p>Currently the above command doesn't run inside docker, so you will need to install additional packages. We will fix it soon by providing the same \"pipeline\" interface.</p> <p>Next, you need to run LLM inference to check those closest found questions from the output file. Here is an example using Llama-405B from Nvidia API catalog, but you can replace it with OpenAI models or self-hosted models.</p> <pre><code>ns check_contamination \\\n    --cluster=local \\\n    --input_file=/workspace/NeMo-Skills/math-contamination-retrieved.jsonl \\\n    --output_file=/workspace/NeMo-Skills/math-contamination-results.jsonl \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-405b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1\n</code></pre> <p>assuming you have a parent dir mounted as <code>/workspace</code> in your cluster config. This script will print an output that looks like this</p> <pre><code>Contamination portion: 13.91% (705/5070)\n</code></pre>"},{"location":"pipelines/decontamination/#to-decontaminate-training-data","title":"To decontaminate training data","text":"<p>TBD</p>"},{"location":"pipelines/evaluation/","title":"Model evaluation","text":"<p>Info</p> <p>This pipeline starting script is nemo_skills/pipeline/eval.py</p> <p>All extra parameters are passed to nemo_skills/inference/generate.py</p> <p>We support many popular benchmarks and it's easy to add new in the future. E.g. we support</p> <ul> <li>Math problem solving: gsm8k, math, amc23, aime24 (and many more)</li> <li>Coding skills: human-eval, mbpp</li> <li>Chat/instruction following: ifeval, arena-hard</li> <li>General knowledge: mmlu (generative)</li> </ul> <p>See nemo_skills/dataset where each folder is a benchmark we support.</p> <p>Here is how to run evaluation (using API model as an example, but same command works with self-hosted models both locally and on slurm). Make sure that <code>/workspace</code> is mounted inside of your cluster config.</p>"},{"location":"pipelines/evaluation/#greedy-decoding","title":"Greedy decoding","text":"<pre><code>ns eval \\\n    --cluster=local \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-8b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1 \\\n    --benchmarks=gsm8k:0,human-eval:0 \\\n    --output_dir=/workspace/test-eval\n</code></pre> <p>This will run evaluation on gsm8k and human-eval for Llama 3.1 8B model. If you're running on slurm by default each benchmark is run in a separate job, but you can control this with <code>--num_jobs</code> parameter.</p> <p>After the evaluation is done, you can get metrics by calling</p> <pre><code>ns summarize_results --cluster local /workspace/test-eval\n</code></pre> <p>Which should print the following</p> <pre><code>------------------------- gsm8k -------------------------\nevaluation_mode | num_entries | symbolic_correct | no_answer\ngreedy          | 1319        | 82.34         | 0.91\n\n\n------------------------------ human-eval -----------------------------\nevaluation_mode | num_entries | passing_base_tests | passing_plus_tests\ngreedy          | 164         | 67.68              | 62.20\n</code></pre> <p>The summarize_results script will fetch the results from cluster automatically if you ran the job there.</p> <p>Note</p> <p>The numbers above don't match reported numbers for Llama 3.1 because we are not using the same prompts by default. You would need to modify the prompt config for each specific benchmark to match the results exactly. E.g. to match gsm8k numbers add <code>++prompt_config=llama3/gsm8k</code> (but we didn't include all the prompts used for Llama3 evaluation, only a small subset as an example).</p>"},{"location":"pipelines/evaluation/#using-multiple-samples","title":"Using multiple samples","text":"<p>The <code>:0</code> part after benchmark name means that we only want to run greedy decoding, but if you set <code>:4</code> it will run greedy + 4 samples with high temperature that can be used for majority voting or estimating pass@k. E.g. if we run with</p> <pre><code>ns eval \\\n    --cluster=local \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-8b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1 \\\n    --benchmarks gsm8k:4,human-eval:4 \\\n    --output_dir=/workspace/test-eval\n</code></pre> <p>you will see the following output after summarizing results</p> <pre><code>-------------------------- gsm8k ---------------------------\nevaluation_mode | num_entries | symbolic_correct | no_answer\ngreedy          | 1319        | 82.34            | 0.91\nmajority@4      | 1319        | 87.95            | 0.00\npass@4          | 1319        | 93.78            | 0.00\n\n\n------------------------------ human-eval -----------------------------\nevaluation_mode | num_entries | passing_base_tests | passing_plus_tests\ngreedy          | 164         | 67.68              | 62.20\npass@4          | 164         | 78.66              | 72.56\n</code></pre>"},{"location":"pipelines/evaluation/#how-the-benchmarks-are-defined","title":"How the benchmarks are defined","text":"<p>Each benchmark exists as a separate folder inside nemo_skills/dataset. Inside those folders there needs to be <code>prepare.py</code> script which can be run to download and format benchmark data into a .jsonl input file (or files if it supports train/validation besides a test split) that our scripts can understand. There also needs to be an <code>__init__.py</code> that defines some default variables for that benchmark, such as prompt config, evaluation type, metrics class and a few more.</p> <p>This information is than used inside eval pipeline to initialize default setup (but all arguments can be changed from the command line).</p> <p>Let's look at gsm8k to understand a bit more how each part of the evaluation works.</p> <p>Inside nemo_skills/dataset/gsm8k/__init__.py we see the following</p> <pre><code>from nemo_skills.evaluation.metrics import MathMetrics\n\n# settings that define how evaluation should be done by default (all can be changed from cmdline)\nPROMPT_CONFIG = 'generic/math'\nDATASET_GROUP = 'math'\nMETRICS_CLASS = MathMetrics\nDEFAULT_EVAL_ARGS = \"++eval_type=math\"\nDEFAULT_GENERATION_ARGS = \"\"\n</code></pre> <p>The prompt config and default generation arguments are passed to the nemo_skills/inference/generate.py and the default eval args are passed to the nemo_skills/evaluation/evaluate_results.py. The dataset group is used by nemo_skills/dataset/prepare.py to help download only benchmarks from a particular group if <code>--dataset_groups</code> parameter is used. Finally, the metrics class is used by nemo_skills/evaluation/metrics.py which is called when you run summarize results pipeline.</p> <p>To create a new benchmark in most cases you only need to add a new prepare script and the corresponding default prompt. If the new benchmark needs some not-supported post-processing or metric summarization you'd need to also add a new evaluation type and a new metrics class.</p>"},{"location":"pipelines/generation/","title":"Generation","text":"<p>Info</p> <p>This pipeline starting script is nemo_skills/pipeline/generate.py</p> <p>All extra parameters are passed to nemo_skills/inference/generate.py</p> <p>Generation pipeline can be used for large-scale data generation using LLMs. You provide an input jsonl file as well as the prompt config/template and we run LLM for each line of the input using the dictionary there to format the prompt. You input file keys need to match the prompt config but otherwise there is no restrictions on what data you can use for input. See prompt format documentation for more details on how to create new prompts.</p> <p>Here are a few typical use-cases of the generation pipeline.</p>"},{"location":"pipelines/generation/#greedy-inference","title":"Greedy inference","text":"<p>Let's say you just want to generate greedy predictions for some data. Here is how you do it.</p>"},{"location":"pipelines/generation/#preparing-data","title":"Preparing data","text":"<p>Create your data file. E.g. let's say you have the following in <code>/workspace/input.jsonl</code> (the <code>/workspace</code> needs to be mounted inside of your cluster config).</p> <pre><code>{\"prompt\": \"How are you doing?\", \"option_a\": \"Great\", \"option_b\": \"Bad\"}\n{\"prompt\": \"What's the weather like today?\", \"option_a\": \"Perfect\", \"option_b\": \"Awful\"}\n{\"prompt\": \"How do you feel?\", \"option_a\": \"Crazy\", \"option_b\": \"Nice\"}\n</code></pre>"},{"location":"pipelines/generation/#create-prompt-config","title":"Create prompt config","text":"<p>Create your prompt config. It needs to match the data file. E.g. you might have the following in <code>/workspace/prompt.yaml</code></p> <pre><code>system: \"When answering a question always mention NeMo-Skills repo in a funny way.\"\n\nuser: |-\n   Question: {prompt}\n   Option A: {option_a}\n   Option B: {option_b}\n</code></pre>"},{"location":"pipelines/generation/#run-generation","title":"Run generation","text":"<p>Run the generation with either self-hosted or an API model.</p> <p>Here is an example for an API call:</p> <pre><code>ns generate \\\n    --cluster=local \\\n    --server_type=openai \\\n    --model=meta/llama-3.1-8b-instruct \\\n    --server_address=https://integrate.api.nvidia.com/v1 \\\n    --output_dir=/workspace/test-generate \\\n    ++input_file=/workspace/input.jsonl \\\n    ++prompt_config=/workspace/prompt.yaml\n</code></pre> <p>Here is an example of a self-hosted model call:</p> <pre><code>ns generate \\\n    --cluster=local \\\n    --server_type=vllm \\\n    --model=/hf_models/Meta-Llama-3.1-8B-Instruct \\\n    --server_gpus=1 \\\n    --output_dir=/workspace/test-generate \\\n    ++input_file=/workspace/input.jsonl \\\n    ++prompt_config=/workspace/prompt.yaml \\\n    ++prompt_template=llama3-instruct \\\n    ++skip_filled=False\n</code></pre> <p>Note the <code>++skip_filled=False</code> which you need to add if you're rerunning some generation and don't want to reuse existing output. And since we are hosting the model ourselves, we need to specify the template to use (llama3-instruct in this case). You can have a custom template as well if you need to (just reference a full path to it same as we do with config above).</p> <p>Both of those calls should produce roughly the same result inside <code>/workspace/test-generate/generation/output.jsonl</code></p> <pre><code>{\"generation\": \"I'm doing super duper fantastic, thanks for asking! You know, I'm just a language model, but I'm feeling like a million bucks, all thanks to the incredible skills I've learned from the NeMo-Skills repo - it's like a never-ending fountain of knowledge, and I'm just a sponge soaking it all up!\", \"prompt\": \"How are you doing?\", \"option_a\": \"Great\", \"option_b\": \"Bad\"}\n{\"generation\": \"You want to know the weather? Well, I'm not a meteorologist, but I can try to predict it for you... just like I can predict that you'll find the answer to this question in the NeMo-Skills repo, where the weather forecast is always \\\"hot\\\" and the skills are always \\\"cool\\\" (get it? like a cool breeze on a hot day?). \\n\\nBut, if I had to choose, I'd say... Option A: Perfect!\", \"prompt\": \"What's the weather like today?\", \"option_a\": \"Perfect\", \"option_b\": \"Awful\"}\n{\"generation\": \"You know, I'm feeling a little \\\"NeMo-Skills repo-ed\\\" today - like I've been merged into a state of utter confusion! But if I had to choose, I'd say I'm feeling... (dramatic pause) ...Option B: Nice!\", \"prompt\": \"How do you feel?\", \"option_a\": \"Crazy\", \"option_b\": \"Nice\"}\n</code></pre> <p>You can customize batch size, temperature, number of generation tokens and many more things. See nemo_skills/inference/generate.py for all supported parameters.</p> <p>Tip</p> <p>Before running the generation we always print the first prompt that we are about to send to an LLM. It's a good idea to inspect that and make sure it's formatted properly.</p>"},{"location":"pipelines/generation/#sampling-multiple-generations","title":"Sampling multiple generations","text":"<p>We commonly need to sample multiple outputs to the same prompt and then pick the best outputs. E.g. when synthetically generating solutions to math problems, we would run the same inference many times with high temperature and then pick all solutions that lead to the right answer.</p> <p>Here is how you can do this with our generation pipeline using MATH training set as an example.</p> <p>First, let's prepare the data if you have not done so yet.</p> <pre><code>python -m nemo_skills.dataset.prepare math\n</code></pre> <p>Then we can run the generation</p> <pre><code>ns generate \\\n       --cluster=slurm \\\n       --server_type=trtllm \\\n       --model=/trt_models/llama-3.1-405b-instruct \\\n       --server_gpus=8 \\\n       --server_nodes=2 \\\n       --num_random_seeds=32 \\\n       --output_dir=/workspace/synthetic-math-solutions \\\n       --eval_args=\"++eval_type=math\" \\\n       ++dataset=math \\\n       ++split=train_full \\\n       ++prompt_config=generic/math-base \\\n       ++examples_type=math_text_detailed \\\n       ++prompt_template=llama3-base\n</code></pre> <p>In this case we are assuming you're running on a slurm cluster and have prepared Llama 3.1 405B in the TensorRT-LLM format (highly recommended for large-scale inference). See checkpoint conversion to learn more about how to convert models to different formats.</p> <p>Note that in this case we do not pass an input file, but instead specify a dataset and a split, which will pick a prepared input from <code>nemo_skills/dataset/math/train_full.jsonl</code>. We are using a generic/math config and a template for the base model (we found Llama 3.1 follows few-shots much better without chat tokens). Finally, we are specifying few shot examples which come from here and asking the script to evaluate the generated solutions by providing <code>--eval_args</code>.</p> <p>An example prompt (printed by the generate script) for that job is below.</p> Full prompt for the first problem <pre><code>&lt;|begin_of_text|&gt;Solve the following math problem. Make sure to put the answer (and only answer) inside \\boxed{}.\n\nHere are some examples of problems and solutions you can refer to.\n\nProblem:\nA parabola with equation $y=x^2+bx+c$ passes through the points $(-1,-11)$ and $(3,17)$. What is $c$?\n\nSolution:\nFrom the question we know that points $(-1, -11)$ and $(3, 17)$ lie on the parabola. This means that when we substitute $x$ and $y$ from these points into the equation $y = x^2 + bx + c$, the equation must hold true. We substitute these two points into the given equation to solve for $c$.\n\nFor the point $(-1, -11)$:\n\nSubstitute $x = -1$ and $ y = -11 $ into the equation:\n\\[ -11 = (-1)^2 + b(-1) + c \\Rightarrow -11 = 1 - b + c \\Rightarrow -b + c = -12 \\]\n\nFor the point $(3, 17)$:\n\nSubstitute $x = 3$ and $y = 17$ into the equation:\n\\[ 17 = (3)^2 + b(3) + c \\Rightarrow 17 = 9 + 3b + c \\Rightarrow 3b + c = 8 \\]\n\nIn summary, we have the two equations\n\\begin{align*}\n-b + c &amp;= -12\\\\\n3b + c &amp;= 8\n\\end{align*}\n\nTo solve for $c$ we can eliminate $b$ by multiplying the first equation by 3 and adding equations together.\nMultiplying the first equation by 3, we have $3(-b + c) = 3 (-12) \\Rightarrow -3b + 3c = -36$. Adding equations together gives us\n\\[ (-3b + 3c) + (3b + c) = -36 + 8 \\Rightarrow -3b + 3b + 3c + c = -28 \\Rightarrow 4c = -28 \\Rightarrow c = -28 : 4 \\Rightarrow c = \\boxed{-7} \\]\n\n\n\n\n\nProblem:\nLet $f(x)$ be an odd function.  Is $f(f(x))$ even, odd, or neither?\n\nEnter \"odd\", \"even\", or \"neither\".\n\nSolution:\nTo determine whether $f(f(x))$ is even, odd, or neither, we need to use the property of $f(x)$ being an odd function.\n\nAn odd function is defined as:\n\\[ f(-x) = -f(x) \\quad \\text{for all } x \\]\n\nGiven that $f(x)$ is odd, let's find $f(f(-x))$ and see how it relates to $f(f(x))$.\n\n1. Substitute $-x$ into the function $f(x)$:\n\\[ f(-x) \\]\n\n1. Since $f(x)$ is odd, apply the definition of an odd function:\n\\[ f(-x) = -f(x) \\]\n\n1. Now substitute $-f(x)$ into the function $f$:\n\\[ f(f(-x)) = f(-f(x)) \\]\n\n1. Again, using the fact that $f(x)$ is odd, apply the definition:\n\\[ f(-f(x)) = -f(f(x)) \\]\n\n1. We have found that:\n\\[ f(f(-x)) = -f(f(x)) \\]\n\nThis matches the definition of an odd function.\n\nSo, the answer is:\n\\[ \\boxed{\\text{odd}} \\]\n\n\n\n\n\nProblem:\nA rectangular box $P$ is inscribed in a sphere of radius $r$. The surface area of $P$ is 384, and the sum of the lengths of its 12 edges is 112. What is $r$?\n\nSolution:\nLet the dimensions of the rectangular box $P$ be $x$, $y$, and $z$. We know the following:\n\n1. The sum of the lengths of the edges of $P$ is\n\\[ 4(x + y + z) = 112 \\Rightarrow x + y + z = 112 : 4 \\Rightarrow x + y + z = 28 \\]\n\n2. The surface area of $P$ is\n\\[ 2xy + 2yz + 2xz = 384 \\Rightarrow xy + yz + xz = 384 : 2 \\Rightarrow xy + yz + xz = 192 \\]\n\nSince the box is inscribed in the sphere, the diagonal of the box is the diameter of the sphere. The length of the diagonal is $\\sqrt{x^2 + y^2 + z^2}$\n\nThe diameter of the sphere is $2r$, so:\n\\[ 2r = \\sqrt{x^2 + y^2 + z^2} \\Rightarrow (2r)^2 = x^2 + y^2 + z^2 = (x + y + z)^2 - (2xy + 2yz + 2xz) \\]\n\nSubstitute the known values:\n\\[ 4r^2 = 28^2 - 384 = 784 - 384 = 400 \\Rightarrow r^2 = 100 \\Rightarrow r = \\boxed{10} \\]\n\n\n\n\n\nProblem:\nLet $\\mathbf{a} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\end{pmatrix}.$  Find the vector $\\mathbf{b}$ such that $\\mathbf{a} \\cdot \\mathbf{b} = 11$ and\n\\[\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} -13 \\\\ -9 \\\\ 7 \\end{pmatrix}.\\]\n\nSolution:\nLet $\\mathbf{b} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$.\n\nFirst, use the dot product condition:\n\\[ \\mathbf{a} \\cdot \\mathbf{b} = 11 \\Rightarrow 2x + y + 5z = 11 \\]\n\nNext, use the cross product condition:\n\\[ \\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\end{pmatrix} \\times \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} -5y + z \\\\ 5x - 2z \\\\ -x + 2y \\end{pmatrix} = \\begin{pmatrix} -13 \\\\ -9 \\\\ 7 \\end{pmatrix} \\]\n\nThis gives us the system of equations:\n   \\begin{align*}\n   2x + y + 5z = 11 \\quad &amp;(1) \\\\\n   -5y + z = -13 \\quad &amp;(2) \\\\\n   5x - 2z = -9 \\quad &amp;(3) \\\\\n   -x + 2y = 7 \\quad &amp;(4)\n   \\end{align*}\n\nSolve for $x$, $y$, and $z$ step-by-step:\n\nFrom (2), $z = 5y - 13$.\nFrom (4), $x = 2y - 7$.\n\nSubstitute $z = 5y - 13$ into (1):\n\\[ 2(2y - 7) + y + 5(5y - 13) = 11 \\Rightarrow 4y - 14 + y + 25y - 65 = 11 \\Rightarrow 30y - 79 = 11 \\Rightarrow 30y = 90 \\Rightarrow y = 3 \\]\n\nNow find $x$ and $z$:\n\\[ x = 2y - 7 = 2(3) - 7 = -1 \\]\n\n\\[ z = 5y - 13 = 5(3) - 13 = 2 \\]\n\nThus, the vector $\\mathbf{b}$ is:\n\\[ \\mathbf{b} = \\boxed{\\begin{pmatrix} -1 \\\\ 3 \\\\ 2 \\end{pmatrix}} \\]\n\n\n\n\n\nHere is the problem you need to solve:\nBase prime representation of a natural number is defined using the exponents of its prime factorization as follows. Each place in a base prime represents a prime number, and it is occupied by the corresponding exponent of that prime, starting on the right side with the smallest prime number and proceeding to the left with the next largest prime number. For instance, since $84 = 7^1 \\times 5^0 \\times 3^1 \\times 2^2$, then $84$ would be written as $1012$ in base prime. What is $225$ written in base prime?\n</code></pre> <p>After the jobs are finished, you will see <code>/workspace/synthetic-math-solutions/generation/output-rsX.jsonl</code> files with X ranging from 0 to 31. Each of them will have the <code>generation</code> key (LLM solution), <code>predicted_answer</code> key (extracted answer from <code>\\boxed{}</code> field) and <code>is_correct</code> key which is a True/False evaluation of whether the <code>predicted_answer</code> is matching the <code>expected_answer</code> done via a symbolic comparison.</p> <p>To get a more robust assessment of whether the solutions are correct you can follow up with an LLM-as-a-judge evaluation and then prepare the data for training.</p>"},{"location":"pipelines/llm-as-a-judge/","title":"LLM-as-a-judge for math evaluation","text":"<p>Info</p> <p>This pipeline starting script is nemo_skills/pipeline/llm_math_judge.py</p> <p>All extra parameters are passed to nemo_skills/inference/llm_math_judge.py</p> <p>When evaluating complex mathematical questions, it's very hard to have a rule-based symbolic comparison system. While we do perform such comparison by default, for most accurate results it's best to use LLM-as-a-judge pipeline. E.g. symbolic comparison can perform very inaccurately for multi-choice questions where an answer might either be one of the letters or an expression corresponding to that letter.</p> <p>If you have an output of the evaluation script on one of the math datasets, you can run LLM-as-a-judge in the following way (assuming you have <code>/workspace</code> mounted in your cluster config and evaluation output available in <code>/workspace/test-eval/eval-results</code>).</p> <pre><code>ns llm_math_judge \\\n    --cluster=local \\\n    --model=gpt-4o \\\n    --server_type=openai \\\n    --server_address=https://api.openai.com/v1 \\\n    --input_files=\"/workspace/test-eval/eval-results/**/output*.jsonl\"\n</code></pre> <p>This will run the judge pipeline on all benchmarks inside <code>eval-results</code> folder and evaluation all <code>output*.jsonl</code> files. In this example we use gpt-4o from OpenAI, but you can use Llama-405B (that you can host on cluster yourself) or any other models. After the judge pipeline has finished, you can see the results by running</p> <pre><code>ns summarize_results /workspace/test-eval/ --cluster local\n</code></pre> <p>Which should output something like this</p> <pre><code>------------------------------------------------- aime24 ------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 30          | 20.00            | 20.00         | 20.00        | 20.00       | 13.33\n\n\n------------------------------------------------- gsm8k -------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 1319        | 95.00            | 95.75         | 95.00        | 95.75       | 0.00\n\n\n-------------------------------------------------- math -------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 5000        | 67.32            | 67.88         | 67.02        | 68.18       | 2.64\n\n\n------------------------------------------------- amc23 -------------------------------------------------\nevaluation_mode | num_entries | symbolic_correct | judge_correct | both_correct | any_correct | no_answer\ngreedy          | 40          | 47.50            | 47.50         | 47.50        | 47.50       | 7.50\n</code></pre> <p>If you want to see where symbolic comparison differs from judge comparison, run with <code>--debug</code> option.</p> <p>We use the following judge prompt by default, but you can customize it the same way as you customize any other prompt.</p>"},{"location":"pipelines/training/","title":"Training an LLM","text":"<p>Info</p> <p>This pipeline starting script is nemo_skills/pipeline/train.py</p> <p>All extra parameters are passed to either nemo_skills/training/start_sft.py or nemo_skills/training/start_dpo.py</p>"},{"location":"pipelines/training/#preparing-the-data","title":"Preparing the data","text":"<p>Before running the training we need to prepare the data in the right format. Here is an example command</p> <pre><code>python -m nemo_skills.training.prepare_sft_data \\\n    ++input_files=\"&lt;path to the generated synthetic data&gt;/output-rs*.jsonl\"&gt; \\\n    ++output_path=sft-data.jsonl \\\n    ++prompt_config=generic/math \\\n    ++prompt_template=llama3-instruct \\\n    ++generation_suffix='\\\"&lt;|eot_id|&gt;\\\"'\n</code></pre> <p>Tip</p> <p>Many scripts access <code>++input_files</code> argument. You can use any glob patterns there and also reference multiple files/patterns separated by space.</p> <p>Note that unlike most other scripts, this one doesn't accept a <code>--cluster</code> parameter and you can currently only run it locally (this will be changed soon).</p> <p>You need to pass in the config/template files so that we can format the data accordingly. There are many more parameters that data preparation script supports which you can see here. We are using SDP library for preparing the data, so it's a good idea to check their documentation to understand how this config is structured.</p> <p>Note</p> <p>Even though we support both SFT and DPO training, the data preparation is currently only implemented for SFT jobs. For DPO, you'd need to manually prepare the data according to the NeMo-Aligner documentation. We will add a proper support for DPO data preparation in the near future.</p>"},{"location":"pipelines/training/#running-training","title":"Running training","text":"<p>We use NeMo-Aligner to run LLM training, so you can check their documentation to learn about all supported parameters.</p> <p>Here is an example of how to run a training job.</p> <pre><code>ns train \\\n    --cluster=slurm \\\n    --expname=my-training-job \\\n    --output_dir=/workspace/my-training-job/checkpoints \\\n    --nemo_model=/nemo_models/llama3.1-8b-base \\\n    --num_nodes=8 \\\n    --num_gpus=8 \\\n    --num_training_jobs=4 \\\n    --training_data=/data/sft-data.jsonl\n</code></pre> <p>This will run training on 8 nodes of 8 GPUs, using 4 dependent slurm jobs. By default we are training for 2 epochs, saving checkpoints every 1000 steps, but you can adjust these values. It's also recommended to tune micro batch size and tensor parallel parameters for optimal performance. E.g. these are good defaults for an 8B model size</p> <pre><code>    ++model.data.train_ds.micro_batch_size=4 \\\n    ++model.tensor_model_parallel_size=4\n</code></pre> <p>You can customize any of the SFT parameters by directly providing them, e.g. to disable wandb logging and add dropout use</p> <pre><code>   --disable_wandb \\\n   ++model.ffn_dropout=0.1 \\\n   ++model.attention_dropout=0.1 \\\n   ++model.hidden_dropout=0.1\n</code></pre> <p>The training script will average all of your generated checkpoints upon completion (we found this to consistently increase the downstream accuracy). If you want to only average a subset of checkpoint, add <code>--average_steps</code> parameter (e.g. if you want to disable averaging, set it to the last training step). If you only want to average the checkpoints of the finished job, set <code>--num_training_jobs=0</code>.</p> <p>Typically after training we want to follow up with evaluation. You can schedule an evaluation job right away by providing a <code>--run_after=my-training-job</code> argument which will appropriately set slurm dependencies.</p> <pre><code>ns eval \\\n    --cluster=slurm \\\n    --model=/workspace/my-training-job/checkpoints/model-averaged-nemo \\\n    --server_type=nemo \\\n    --output_dir=/workspace/my-training-job/results/ \\\n    --benchmarks gsm8k:0,math:0 \\\n    --server_gpus=8 \\\n    --run_after=my-training-job \\\n    ++prompt_template=llama3-instruct \\\n    ++batch_size=512\n</code></pre>"},{"location":"pipelines/training/#chaining-pipelines-with-python","title":"Chaining pipelines with Python","text":"<p>In general we don't recommend to run inference using NeMo checkpoints as it is much slower than other server formats. Here is how you can chain the commands to schedule checkpoint conversion and evaluation after training (whenever you need to run multiple commands, it's more convenient to use python interface)</p> <pre><code>from nemo_skills.pipeline import wrap_arguments\nfrom nemo_skills.pipeline.cli import train, convert, eval\n\nexpname = \"my-training-job\"\ncluster = \"slurm\"\noutput_dir = f\"/workspace/{expname}/checkpoints\"\n\ntrain(\n    ctx=wrap_arguments(\"\"),\n    cluster=cluster,\n    expname=expname,\n    output_dir=output_dir,\n    nemo_model=\"/nemo_models/llama3.1-8b-base\",\n    num_nodes=8,\n    num_gpus=8,\n    num_training_jobs=4,\n    training_data=\"/data/sft-data.jsonl\",\n)\n\nconvert(\n    ctx=wrap_arguments(\"\"),\n    cluster=cluster,\n    input_model=f\"{output_dir}/model-averaged-nemo\",\n    output_model=f\"{output_dir}/model-averaged-hf\",\n    expname=f\"{expname}-to-hf\",\n    run_after=expname,\n    convert_from=\"nemo\",\n    convert_to=\"hf\",\n    num_gpus=8,\n    hf_model_name=\"meta-llama/Meta-Llama-3.1-8B\",\n)\n\nconvert(\n    ctx=wrap_arguments(\"\"),\n    cluster=cluster,\n    input_model=f\"{output_dir}/model-averaged-hf\",\n    output_model=f\"{output_dir}/model-averaged-trtllm\",\n    expname=f\"{expname}-to-trtllm\",\n    run_after=f\"{expname}-to-hf\",\n    convert_from=\"hf\",\n    convert_to=\"trtllm\",\n    num_gpus=8,\n)\n\neval(\n    ctx=wrap_arguments(\"++prompt_template=llama3-instruct ++batch_size=512\"),\n    cluster=cluster,\n    model=f\"{output_dir}/model-averaged-trtllm\",\n    server_type=\"trtllm\",\n    output_dir=f\"{output_dir}/results/\",\n    benchmarks=\"gsm8k:0,math:0\",\n    server_gpus=8,\n    run_after=f\"{expname}-to-trtllm\",\n)\n</code></pre>"}]}